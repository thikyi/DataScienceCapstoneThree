{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8edaf90",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e005add",
   "metadata": {},
   "source": [
    "### Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6deff0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from scipy.stats import randint\n",
    "from sklearn import __version__ as sklearn_version\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import (make_scorer,accuracy_score, auc, classification_report, confusion_matrix,\n",
    "                             ConfusionMatrixDisplay, f1_score, mean_absolute_error,\n",
    "                             mean_squared_error, precision_score, r2_score, recall_score, roc_curve,roc_auc_score)\n",
    "from sklearn.model_selection import (GridSearchCV, RandomizedSearchCV, cross_val_score,\n",
    "                                     cross_validate, learning_curve, train_test_split)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c646dd",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d783fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafilepath = \"../data/interim/train_test_split.pkl\"\n",
    "\n",
    "with open(datafilepath, 'rb') as file:\n",
    "    X,y,X_train, X_test, y_train, y_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d02d786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MonthlyRevenue</th>\n",
       "      <th>MonthlyMinutes</th>\n",
       "      <th>TotalRecurringCharge</th>\n",
       "      <th>DirectorAssistedCalls</th>\n",
       "      <th>OverageMinutes</th>\n",
       "      <th>RoamingCalls</th>\n",
       "      <th>CustomerCareCalls</th>\n",
       "      <th>IncomeGroup</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>MaritalStatus_No</th>\n",
       "      <th>...</th>\n",
       "      <th>CityCode_SEW</th>\n",
       "      <th>CityCode_SFR</th>\n",
       "      <th>CityCode_SFU</th>\n",
       "      <th>CityCode_SHE</th>\n",
       "      <th>CityCode_SLC</th>\n",
       "      <th>CityCode_SLU</th>\n",
       "      <th>CityCode_STL</th>\n",
       "      <th>CityCode_VAH</th>\n",
       "      <th>ChildrenInHH_No</th>\n",
       "      <th>ChildrenInHH_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>49.01</td>\n",
       "      <td>220.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14784</th>\n",
       "      <td>166.82</td>\n",
       "      <td>706.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.97</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10258</th>\n",
       "      <td>35.45</td>\n",
       "      <td>215.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44546</th>\n",
       "      <td>10.20</td>\n",
       "      <td>39.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48450</th>\n",
       "      <td>58.80</td>\n",
       "      <td>708.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21243</th>\n",
       "      <td>52.63</td>\n",
       "      <td>526.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45891</th>\n",
       "      <td>29.99</td>\n",
       "      <td>113.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42613</th>\n",
       "      <td>39.30</td>\n",
       "      <td>166.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43567</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>51.73</td>\n",
       "      <td>109.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40837 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MonthlyRevenue  MonthlyMinutes  TotalRecurringCharge  \\\n",
       "10995           49.01           220.0                  24.0   \n",
       "14784          166.82           706.0                  60.0   \n",
       "10258           35.45           215.0                  45.0   \n",
       "44546           10.20            39.0                  10.0   \n",
       "48450           58.80           708.0                  70.0   \n",
       "...               ...             ...                   ...   \n",
       "21243           52.63           526.0                  60.0   \n",
       "45891           29.99           113.0                  30.0   \n",
       "42613           39.30           166.0                  40.0   \n",
       "43567           10.00             0.0                  10.0   \n",
       "2732            51.73           109.0                  50.0   \n",
       "\n",
       "       DirectorAssistedCalls  OverageMinutes  RoamingCalls  CustomerCareCalls  \\\n",
       "10995                   0.00             0.0           0.0                3.7   \n",
       "14784                   2.97           153.0           0.0                3.3   \n",
       "10258                   0.00             0.0           0.0                0.0   \n",
       "44546                   0.00             0.0           0.1                0.0   \n",
       "48450                   0.00             0.0           0.8                2.3   \n",
       "...                      ...             ...           ...                ...   \n",
       "21243                   0.00             0.0           2.6                0.0   \n",
       "45891                   0.00             0.0           0.0                0.0   \n",
       "42613                   1.49            10.0           1.7                0.3   \n",
       "43567                   0.00             0.0           0.0                0.0   \n",
       "2732                    0.99             1.0           0.0                0.0   \n",
       "\n",
       "       IncomeGroup  Cluster  MaritalStatus_No  ...  CityCode_SEW  \\\n",
       "10995            0        0               0.0  ...           0.0   \n",
       "14784            0        2               0.0  ...           0.0   \n",
       "10258            0        0               0.0  ...           0.0   \n",
       "44546            1        0               0.0  ...           0.0   \n",
       "48450            0        0               0.0  ...           0.0   \n",
       "...            ...      ...               ...  ...           ...   \n",
       "21243            4        0               1.0  ...           0.0   \n",
       "45891            8        0               0.0  ...           0.0   \n",
       "42613            7        0               0.0  ...           0.0   \n",
       "43567            0        0               0.0  ...           0.0   \n",
       "2732             9        1               0.0  ...           0.0   \n",
       "\n",
       "       CityCode_SFR  CityCode_SFU  CityCode_SHE  CityCode_SLC  CityCode_SLU  \\\n",
       "10995           0.0           0.0           0.0           0.0           0.0   \n",
       "14784           0.0           0.0           0.0           0.0           0.0   \n",
       "10258           0.0           0.0           0.0           0.0           0.0   \n",
       "44546           0.0           0.0           0.0           0.0           0.0   \n",
       "48450           0.0           0.0           0.0           0.0           0.0   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "21243           0.0           0.0           0.0           0.0           0.0   \n",
       "45891           0.0           0.0           0.0           0.0           0.0   \n",
       "42613           0.0           0.0           0.0           0.0           0.0   \n",
       "43567           0.0           0.0           0.0           0.0           0.0   \n",
       "2732            0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "       CityCode_STL  CityCode_VAH  ChildrenInHH_No  ChildrenInHH_Yes  \n",
       "10995           0.0           0.0              1.0               0.0  \n",
       "14784           0.0           0.0              1.0               0.0  \n",
       "10258           0.0           0.0              1.0               0.0  \n",
       "44546           0.0           0.0              1.0               0.0  \n",
       "48450           0.0           0.0              1.0               0.0  \n",
       "...             ...           ...              ...               ...  \n",
       "21243           0.0           0.0              1.0               0.0  \n",
       "45891           0.0           0.0              1.0               0.0  \n",
       "42613           0.0           0.0              1.0               0.0  \n",
       "43567           0.0           0.0              1.0               0.0  \n",
       "2732            0.0           0.0              1.0               0.0  \n",
       "\n",
       "[40837 rows x 79 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2fb903",
   "metadata": {},
   "source": [
    "#### Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3f4d007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2896392976957171"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the mean of `y_train`\n",
    "train_mean = y_train.mean()\n",
    "train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f213f076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MonthlyRevenue            48.46\n",
       "MonthlyMinutes           366.00\n",
       "TotalRecurringCharge      45.00\n",
       "DirectorAssistedCalls      0.25\n",
       "OverageMinutes             3.00\n",
       "                          ...  \n",
       "CityCode_SLU               0.00\n",
       "CityCode_STL               0.00\n",
       "CityCode_VAH               0.00\n",
       "ChildrenInHH_No            1.00\n",
       "ChildrenInHH_Yes           0.00\n",
       "Length: 79, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the medium of `y_train`\n",
    "X_defaults_median = X_train.median()\n",
    "X_defaults_median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25afc181",
   "metadata": {},
   "source": [
    "#### Impute NaN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137b2d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in X_test: 0\n",
      "NaN values in X: 0\n",
      "Infinity values in X_test: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "print(\"NaN values in X_test:\", X_test.isna().sum().sum())\n",
    "print(\"NaN values in X:\", X.isna().sum().sum())\n",
    "\n",
    "# Replace NaN values with the mean (you can choose other methods)\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "#X=X.fillna(X.mean())\n",
    "\n",
    "# Check for infinity values\n",
    "print(\"Infinity values in X_test:\", np.isinf(X_test).sum().sum())\n",
    "#print(\"Infinity values in X:\", np.isinf(X).sum().sum())\n",
    "\n",
    "# Replace infinity values\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(X_test.mean())\n",
    "#X= X.replace([np.inf, -np.inf], np.nan).fillna(X.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b285e727",
   "metadata": {},
   "source": [
    "#### Normalize or Scale your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e06f3ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()\n",
    "#X_test = scaler.fit_transform(X_test)\n",
    "#X_test\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f206d",
   "metadata": {},
   "source": [
    "##### Handling imbalance data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7cdf7",
   "metadata": {},
   "source": [
    "As dataset contains more Non-churn customers,One of the resampling techniques will be considered to use to balance out the data for both Churn and Non-Churn categories. It will help the prediction results in a more balanced dataset that allows the model to learn more effectively from both classes, improving its accuracy and predictive performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e81ba6",
   "metadata": {},
   "source": [
    "Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2533466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ors = SMOTE(random_state=42)\n",
    "#For traning set\n",
    "#X_train_Oresampled, y_train_Oresampled = ors.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50119aaa",
   "metadata": {},
   "source": [
    "Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27eaf442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#urs = RandomUnderSampler(random_state=42)\n",
    "#For traning set\n",
    "#X_train_Uresampled, y_train_Uresampled = urs.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4edec5",
   "metadata": {},
   "source": [
    "Combine both Under & Over Sampling (Synthetic Minority Over-sampling Technique + Edited Nearest Neighbors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b630f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crs= SMOTEENN(random_state=42)\n",
    "#X_train_Cresampled, y_train_Cresampled = crs.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25a27841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6243d2b",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b87ddd",
   "metadata": {},
   "source": [
    "#### Initialize models with hyper-parameter tuning and resampling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c435d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Decision Tree\": (DecisionTreeClassifier(random_state=0), {\"max_depth\": [3, 5, 10]}),\n",
    "    \"Random Forest\": (RandomForestClassifier(random_state=0), {\"n_estimators\": [10, 50, 100]}),\n",
    "    \"Gradient Boosting\": (GradientBoostingClassifier(random_state=0), {\"learning_rate\": [0.01, 0.1, 0.2]}),\n",
    "    \"Neural Network\": (MLPClassifier(max_iter=1000,tol=1e-4), {\"hidden_layer_sizes\": [(50,), (100,)]}),\n",
    "    \"KNN\": (KNeighborsClassifier(), {\"n_neighbors\": [3, 5, 7]}),\n",
    "    \"XGBoost\": (XGBClassifier(), {\"n_estimators\": [100, 200], \"learning_rate\": [0.01, 0.1]})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e4886e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models2 = {\n",
    "    \"Decision Tree\": (DecisionTreeClassifier(random_state=0), [{\"max_depth\": 3}, {\"max_depth\": 5}, {\"max_depth\": 10}]),\n",
    "    \"Random Forest\": (RandomForestClassifier(random_state=0), [{\"n_estimators\": 10}, {\"n_estimators\": 50}, {\"n_estimators\": 100}]),\n",
    "    \"Gradient Boosting\": (GradientBoostingClassifier(random_state=0), [{\"learning_rate\": 0.01}, {\"learning_rate\": 0.1}, {\"learning_rate\": 0.2}]),\n",
    "    \"Neural Network\": (MLPClassifier(max_iter=400), [{\"hidden_layer_sizes\": (50,)}, {\"hidden_layer_sizes\": (100,)}]),\n",
    "    \"KNN\": (KNeighborsClassifier(), [{\"n_neighbors\": 3}, {\"n_neighbors\": 5}, {\"n_neighbors\": 7}]),\n",
    "    \"XGBoost\": (XGBClassifier(), [{\"n_estimators\": 100, \"learning_rate\": 0.01}, {\"n_estimators\": 200, \"learning_rate\": 0.1}])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468f9d1",
   "metadata": {},
   "source": [
    "<b>Handling Imbalance Data </b></br> As dataset contains more Non-churn customers,One of the resampling techniques will be considered to use to balance out the data for both Churn and Non-Churn categories. It will help the prediction results in a more balanced dataset that allows the model to learn more effectively from both classes, improving its accuracy and predictive performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bad2eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampling_methods = {\n",
    "    \"Under Sampling\": RandomUnderSampler(random_state=0),\n",
    "    \"SMOTEENN\": SMOTEENN(random_state=0),\n",
    "    \"Over Sampling\": RandomOverSampler(random_state=0),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203785ef",
   "metadata": {},
   "source": [
    "#### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ae5e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f691ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, params, X_train, y_train, X_test, y_test):\n",
    "    scoring = {'AUC': make_scorer(roc_auc_score)}\n",
    "    try:\n",
    "        grid_search = GridSearchCV(model, params, cv=5, scoring=scoring, refit='AUC')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "    except TypeError as e:\n",
    "        print(\"Error occurred while performing GridSearchCV:\", e)\n",
    "        print(\"Skipping this iteration and continuing with the next one.\")\n",
    "        if model_name == 'Decision Tree':\n",
    "            params = {'max_depth': 10}  # Correcting the params for Decision Tree\n",
    "            grid_search = GridSearchCV(model, params, cv=5, scoring=scoring, refit='AUC')\n",
    "            grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_scores = best_model.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_scores)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Classification Report\": class_report,\n",
    "        \"CV Score\": cv_scores.mean(),\n",
    "        \"ROC AUC\": roc_auc,\n",
    "        \"Confusion Matrix\": cm,\n",
    "        \"GridSearch Best Score\": grid_search.best_score_,\n",
    "        \"GridSearch Best Param\": grid_search.best_params_\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e2020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through models\n",
    "for resampling_name, resampler in resampling_methods.items():\n",
    "    start_time = time.time()\n",
    "    current_time = datetime.now()\n",
    "    X_resampled, y_resampled = resampler.fit_resample(X_train_scaled, y_train)\n",
    "    print('Resampling Technique: ', resampling_name)\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    model_performance = Parallel(n_jobs=-1)(\n",
    "    delayed(train_model)(model, params, X_resampled, y_resampled, X_test_scaled, y_test) \n",
    "    for model_name, (model, params) in models.items()\n",
    "    )\n",
    "    \n",
    "    print('----------------------------------')\n",
    "    elapsed_time = (time.time() - start_time) / 60\n",
    "    print(\"Elapsed time: {:.2f} minutes\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd845d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2690121112.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/pk/m2_8c77j2zx4w04mcryfb0z00000gn/T/ipykernel_29588/2690121112.py\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    params = {'max_depth': 10}  # Correcting the params for Decision Tree\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Loop through models\n",
    "for resampling_name, resampler in resampling_methods.items():\n",
    "    print('Resampling Technique: ', resampling_name)\n",
    "    print('----------------------------------')\n",
    "    X_resampled, y_resampled = resampler.fit_resample(X_train_scaled, y_train)\n",
    "    \n",
    "    for name,(model, params) in models.items():\n",
    "        start_time = time.time()\n",
    "        current_time = datetime.now()\n",
    "        print('Started at: ', current_time.strftime(\"%H:%M:%S\"))\n",
    "        print('Model Name: ',name)\n",
    "        print('----------------------------------')\n",
    "    \n",
    "        # Define scoring method for GridSearchCV, e.g., ROC AUC for imbalanced data\n",
    "        scoring = {'AUC': make_scorer(roc_auc_score)}\n",
    "        \n",
    "        # Perform GridSearchCV\n",
    "        try:\n",
    "            # Perform GridSearchCV\n",
    "            grid_search = GridSearchCV(model, params, cv=5, scoring=scoring, refit='AUC')\n",
    "            grid_search.fit(X_resampled, y_resampled)\n",
    "        except TypeError as e:\n",
    "            print(\"Error occurred while performing GridSearchCV:\", e)\n",
    "            print(\"Skipping this iteration and continuing with the next one.\")\n",
    "            if name == 'Decision Tree':\n",
    "                params = {'max_depth': 10}  # Correcting the params for Decision Tree\n",
    "                grid_search = GridSearchCV(model, params, cv=5, scoring=scoring, refit='AUC')\n",
    "                grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "        grid_search.fit(X_resampled, y_resampled)\n",
    "        grid_search.fit(X_resampled, y_resampled)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        params[name] = grid_search.best_params_\n",
    "        best_model = grid_search.best_estimator_\n",
    "    \n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        y_scores = best_model.predict_proba(X_test_scaled)[:, 1]  # Get the probabilities for the positive class\n",
    "        roc_auc = roc_auc_score(y_test, y_scores)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Store model performance\n",
    "        model_performance[name] = {\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Classification Report\": class_report,\n",
    "            \"CV Score\": cv_scores.mean(),\n",
    "            \"ROC AUC\": roc_auc,\n",
    "            \"Confusion Matrix\": cm,\n",
    "            \"GridSearch Best Score\": grid_search.best_score_,\n",
    "            \"GridSearch Best Param\": grid_search.best_params_,\n",
    "            \"Resampling\": resampling_name\n",
    "        }\n",
    "      \n",
    "        print('----------------------------------')\n",
    "        elapsed_time = (time.time() - start_time) / 60\n",
    "        print(\"Elapsed time: {:.2f} minutes\".format(elapsed_time))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d753d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, params, X, y):\n",
    "    grid_search = GridSearchCV(model, params, cv=5, scoring=scoring, refit='AUC')\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3988bb8",
   "metadata": {},
   "source": [
    "##### Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8189b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, performance in model_performance.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {performance['Accuracy']}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(performance['Classification Report'])\n",
    "    print(\"cv scores:\")\n",
    "    print(performance['CV Score'])\n",
    "    print(\"ROC AUC score:\")\n",
    "    print(performance['ROC AUC'])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(performance['Confusion Matrix'])\n",
    "    print(\"GridSearch Best Score:\")\n",
    "    print(performance['GridSearch Best Score'])\n",
    "    print(\"GridSearch Best Param:\")\n",
    "    print(performance['GridSearch Best Param'])\n",
    "    print(\"Resampling:\")\n",
    "    print(performance['Resampling'])\n",
    "    print(\"-\" * 80)  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a21ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the relevant metrics for DataFrame creation\n",
    "models, accuracies, precisions, recalls, f1_scores , roc_auc , sampling ,bestparam, bestscore  = [], [], [], [], [] ,[],[],[],[]\n",
    "\n",
    "for model, metrics in model_performance.items():\n",
    "    models.append(model)\n",
    "    \n",
    "    accuracies.append(metrics['Accuracy'])\n",
    "    lines = metrics['Classification Report'].split('\\n')\n",
    "    summary_metrics = lines[-2].split()\n",
    "    precisions.append(float(summary_metrics[2]))\n",
    "    recalls.append(float(summary_metrics[3]))\n",
    "    f1_scores.append(float(summary_metrics[4]))\n",
    "    roc_auc.append(metrics['ROC AUC'])\n",
    "    sampling.append(metrics['Resampling'])\n",
    "    bestparam.append(metrics['GridSearch Best Param'])\n",
    "    bestscore.append(metrics['GridSearch Best Score'])\n",
    "\n",
    "# Creating the DataFrame\n",
    "df_report = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Accuracy': accuracies,\n",
    "    'Precision': precisions,\n",
    "    'Recall': recalls,\n",
    "    'F1-Score': f1_scores,\n",
    "    'ROCAUC-Score': roc_auc,\n",
    "    'ResamplingTech': sampling,\n",
    "    'BestParam': bestparam,\n",
    "    'BestScore': bestscore\n",
    "   \n",
    "})\n",
    "\n",
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e46a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "# Accuracy Plot\n",
    "df_report.plot(kind='bar', x='Model', y='Accuracy', ax=axs[0], color='skyblue', hue='ResamplingTech', legend=False)\n",
    "axs[0].set_title('Model Accuracy Comparison')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_ylim(0, 1)\n",
    "axs[0].set_xticklabels(axs[0].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Precision, Recall, F1-Score Plot\n",
    "df_report.plot(kind='bar', x='Model', y=['Precision', 'Recall', 'F1-Score', 'BestScore', 'ROCAUC-Score'], ax=axs[1], hue='ResamplingTech')\n",
    "axs[1].set_title('Model Precision, Recall, F1-Score Comparison')\n",
    "axs[1].set_ylabel('Score')\n",
    "axs[1].set_ylim(0, 1)\n",
    "axs[1].set_xticklabels(axs[1].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84343991",
   "metadata": {},
   "source": [
    "<b>The Gradient Boosting model</b> is the best performer in terms of accuracy , making it a strong candidate for selection. It shows a good balance between precision and recall, especially for class 0 (the majority class).\n",
    "</br>\n",
    "Accuracy: 58 %, the highest among the models. </br>\n",
    "ROC AUC: 60% , showing some ability to discriminate but still limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87d37d",
   "metadata": {},
   "source": [
    "##### Lets see cross validation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc37315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_names = list(model_performance.keys())\n",
    "cv_scores = [model_performance[model]['CV Score'] for model in model_names]\n",
    "resampling_techs = [model_performance[model]['Resampling'] for model in model_names]\n",
    "\n",
    "\n",
    "cv_means = [np.mean(scores) for scores in cv_scores]\n",
    "cv_stds = [np.std(scores) for scores in cv_scores]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([f\"{model}\\n({resampling})\" for model, resampling in zip(model_names, resampling_techs)], cv_means, yerr=cv_stds, capsize=5, color='skyblue')\n",
    "plt.xlabel('Model (Resampling Technique)')\n",
    "plt.ylabel('Mean CV Score')\n",
    "plt.title('Cross-Validation Score Comparison')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "list(zip(model_names, resampling_techs, cv_means, cv_stds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3095de75",
   "metadata": {},
   "source": [
    "Again,<b>Gradient Boosting </b> has Highest performance & consistency with mean score 0.714,and  is the most suitable model for this dataset, showing the best average cross-validation score and consistency. It is closely followed by Random Forest and XGBoost, which also exhibit strong performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10821ad5",
   "metadata": {},
   "source": [
    "### Final Best Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec7698",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_model = GradientBoostingClassifier(random_state=0)\n",
    "#final_model = KNeighborsClassifier()\n",
    "name = \"Gradient Boosting Classifier\"\n",
    "\n",
    "# Adding model metadata\n",
    "final_model.version = '1.0'\n",
    "final_model.pandas_version = pd.__version__\n",
    "final_model.numpy_version = np.__version__\n",
    "final_model.sklearn_version = sklearn_version\n",
    "final_model.X_columns = [col for col in X_train.columns] \n",
    "final_model.build_datetime = datetime.now()\n",
    "\n",
    "start_time = time.time()\n",
    "current_time = datetime.now()\n",
    "print('Started at: ', current_time.strftime(\"%H:%M:%S\"))\n",
    "print('Model Name: ',name)\n",
    "print('----------------------------------')\n",
    "    \n",
    "\n",
    "#final_model.fit(np.vstack((X_train, X_test)), np.concatenate((y_train, y_test)))\n",
    "final_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "final_pred = final_model.predict(scaler.fit_transform(X))\n",
    "final_y_prob = final_model.predict_proba(scaler.fit_transform(X))[:, 1]  # Probability of class 1 (churn)\n",
    "final_roc_auc = roc_auc_score(y, final_y_prob)\n",
    "final_accuracy = accuracy_score(y, final_pred)\n",
    "final_class_report = classification_report(y, final_pred)\n",
    "final_cv_scores = cross_val_score(final_model, X, y, cv=5)\n",
    "final_cm = confusion_matrix(y,final_pred)\n",
    "\n",
    "final_model.cv_scores = final_cv_scores\n",
    "final_model.mean_cv_score = final_cv_scores.mean()\n",
    "\n",
    "print(f'CV Scores: {final_cv_scores}')\n",
    "print(f'Average CV Score: {final_cv_scores.mean()}')\n",
    "print(f'Accurancy: {final_accuracy}')\n",
    "print(f'ROC AUC: {final_roc_auc}')\n",
    "print(f'Confusion Matrix: {final_cm}')\n",
    "print(f'Report: {final_class_report}')\n",
    "\n",
    "\n",
    "print('----------------------------------')\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print(\"Elapsed time: {:.2f} minutes\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2fa4d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize Feature Importance\n",
    "feature_importance = final_model.feature_importances_\n",
    "feature_names = X_train.columns  # Adjust this based on your feature names\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.barplot(x=feature_importance[indices], y=feature_names[indices])\n",
    "plt.title(f'{name} :Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229e1183",
   "metadata": {},
   "source": [
    "<b>Confusion Matrix</b> </br>\n",
    "True Negative (TN): cm[0, 0] - Actual negatives that were predicted as negative</br>\n",
    "False Positive (FP): cm[0, 1] - Actual negatives that were predicted as positive</br>\n",
    "False Negative (FN): cm[1, 0] - Actual positives that were predicted as negative</br>\n",
    "True Positive (TP): cm[1, 1] - Actual positives that were predicted as positive</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2251365f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(final_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(f'{name} Confusion Matrix')  \n",
    "plt.xlabel('Predicted labels') \n",
    "plt.ylabel('True labels')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c8bc5",
   "metadata": {},
   "source": [
    "xAbove Confusion Matrix suggests that the classifier has a high number of false positives, which could be indicative of a low precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d3a62",
   "metadata": {},
   "source": [
    "The accuracy is moderately high, indicating that the classifier is correct about 58.83% of the time.\n",
    "The precision is relatively low, meaning that when the classifier predicts an instance as positive, it is only correct about 35.88% of the time.\n",
    "The recall is higher than precision, which indicates that the classifier is reasonably good at detecting positive instances, identifying about 54.45% of all actual positives.\n",
    "The F1 score, which balances precision and recall, is not very high, suggesting that the classifier does not achieve an excellent balance between precision and recall.\n",
    "The specificity is moderately high, indicating that the classifier is fairly good at identifying negative instances, being correct about 60.61% of the time when predicting the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902cbae9",
   "metadata": {},
   "source": [
    "Lets see ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ffe45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y, final_y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b3d337",
   "metadata": {},
   "source": [
    "xROC curve with an AUC of 0.52, the predictive model has limited discrimination ability and is only marginally better than random guessing.This suggests that the model is not performing well at distinguishing between the positive and negative classes. Improvements to the model, features, or data quality might be necessary to achieve better classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a6d08e",
   "metadata": {},
   "source": [
    "The AUC value ranges from 0 to 1:\n",
    "\n",
    "An AUC of 0.5 suggests no discriminative ability, equivalent to random guessing.\n",
    "An AUC of 1.0 represents perfect discrimination, where the classifier can perfectly differentiate between the two classes.\n",
    "An AUC less than 0.5 suggests worse than random predictions, but this is typically observed only when there's a problem with the way the classifier is being used.\n",
    "In your case, the AUC is 0.61, which suggests that the classifier does better than random guessing, but there's still a lot of room for improvement. Typically, an AUC of 0.7 to 0.8 is considered acceptable, while an AUC above 0.8 indicates a very good classifier, depending on the context and application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17212418",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred =X.copy()\n",
    "df_pred['Churn_Predicted'] = final_pred\n",
    "df_pred['Churn'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae29e79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pred[df_pred['ChurnPredicted']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2522a69e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = df_pred['ChurnPredicted'].value_counts().sort_index()\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d1a2f3",
   "metadata": {},
   "source": [
    "##### Save Model File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_modelfilepath = \"../models/final_model.pkl\"\n",
    "with open(save_modelfilepath, 'wb') as file:\n",
    "    pickle.dump([final_model], file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e96a16a",
   "metadata": {},
   "source": [
    "#### Post-Classification Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fe7ed7",
   "metadata": {},
   "source": [
    "Using churn probabilities from the classification model to further segment customers based on their risk of churning. This segmentation can help in strategizing targeted interventions for different risk groups. </br>\n",
    "Low Risk (p < 0.3)</br>\n",
    "Medium Risk (0.3 ≤ p < 0.7)</br>\n",
    "High Risk (p ≥ 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b220a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_segments = np.digitize(final_y_prob, bins=[0.3, 0.7])\n",
    "\n",
    "#X_risk = X.copy()\n",
    "\n",
    "df_pred['ChurnProbability'] = final_y_prob\n",
    "df_pred['RiskSegment'] = risk_segments\n",
    "\n",
    "# Analyze the distribution of risk segments\n",
    "risk_segment_distribution = df_pred['RiskSegment'].value_counts().sort_index()\n",
    "risk_segment_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d380a00",
   "metadata": {},
   "source": [
    "### Recommandation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0819cd0",
   "metadata": {},
   "source": [
    "Implement Targeted Strategies Based on Risk Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31b3647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_targeted_strategies(df):\n",
    "    # Define strategies for each risk segment\n",
    "    strategies = {\n",
    "        0: \"Send personalized emails offering discounts or special offers.\",\n",
    "        1: \"Engage with customer service for feedback and improvement suggestions.\",\n",
    "        2: \"Offer loyalty programs or benefits to enhance customer retention.\"\n",
    "    }\n",
    "    \n",
    "    df['RetentionStrategy'] = df['RiskSegment'].map(strategies)\n",
    "    \n",
    "    # Example: Executing strategies (this could be more complex in practice)\n",
    "    for segment, strategy in strategies.items():\n",
    "        customers = df[df['RiskSegment'] == segment]\n",
    "        print(f\"Segment {segment}: {len(customers)} customers\")\n",
    "        print(f\"Strategy: {strategy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc3114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "implement_targeted_strategies(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0064a547",
   "metadata": {},
   "source": [
    "### Encode the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fc69e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume that the prefix before the last underscore in each column name represents the original variable name\n",
    "# Extract the prefix of each one-hot encoded column\n",
    "\n",
    "encoded_columns = [col for col in df_pred.columns if '_' in col]  # Consider only columns with '_' in their names\n",
    "prefixes = set(col.rsplit('_', 1)[0] for col in encoded_columns)\n",
    "categorical_mappings = {prefix: [col for col in encoded_columns if col.startswith(prefix)] for prefix in prefixes}\n",
    "\n",
    "categorical_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75297ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding the one-hot encoded columns into the original categorical data\n",
    "decoded_data = df_pred.copy()\n",
    "\n",
    "# Note: 'Churn' mapping was mistakenly captured and will be disregarded for the main issue\n",
    "excluded_churn = {k: v for k, v in categorical_mappings.items() if k != 'Churn'}\n",
    "decoded_columns = []\n",
    "\n",
    "for prefix, columns in excluded_churn.items():\n",
    "    column_name = prefix\n",
    "    decoded_data[column_name] = decoded_data[columns].idxmax(axis=1).str.replace(f'{prefix}_', '')\n",
    "    decoded_columns.append(column_name)\n",
    "    \n",
    "\n",
    "encoded_columns_to_drop = [col for cols in excluded_churn.values() for col in cols]\n",
    "decoded_data_preview = decoded_data.drop(columns=encoded_columns_to_drop)\n",
    "decoded_data_preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb6781",
   "metadata": {},
   "outputs": [],
   "source": [
    "preddata_filepath = \"../data/processed/pred_result.csv\"\n",
    "decoded_data_preview.to_csv(preddata_filepath, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f200b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
